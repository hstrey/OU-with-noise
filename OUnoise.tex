% ****** Start of file OUnoise.tex ******
%%
\documentclass[%
 reprint,
%superscriptaddress,
%groupedaddress,
%unsortedaddress,
%runinaddress,
%frontmatterverbose, 
%preprint,
%showpacs,preprintnumbers,
%nofootinbib,
%nobibnotes,
%bibnotes,
 amsmath,amssymb,
 aps,
%pra,
%prb,
%rmp,
%prstab,
%prstper,
%floatfix,
]{revtex4-1}

\usepackage{graphicx}% Include figure files
\usepackage{dcolumn}% Align table columns on decimal point
\usepackage{bm}% bold math
\usepackage{subcaption}
\usepackage{float}
%\usepackage{hyperref}% add hypertext capabilities
%\usepackage[mathlines]{lineno}% Enable numbering of text and display math
%\linenumbers\relax % Commence numbering lines

%\usepackage[showframe,%Uncomment any one of the following lines to test 
%%scale=0.7, marginratio={1:1, 2:3}, ignoreall,% default settings
%%text={7in,10in},centering,
%%margin=1.5in,
%%total={6.5in,8.75in}, top=1.2in, left=0.9in, includefoot,
%%height=10in,a5paper,hmargin={3cm,0.8in},
%]{geometry}
\DeclareMathOperator\erf{erf}
\DeclareMathOperator\erfc{erfc}
\begin{document}

\preprint{APS/123-QED}

\title{Parameter estimation from an Ornstein-Uhlenbeck process with measurement noise}

\author{Helmut H. Strey}
 \affiliation{Biomedical Engineering Department and Laufer Center for Physical and Quantitative Biology, Stony Brook University, Stony Brook NY 11794-5281.}%Lines break automatically or can be forced with \\

\date{\today}% It is always \today, today,
             %  but any date may be explicitly specified

\begin{abstract}
\begin{description}
\item[PACS numbers]
May be entered using the \verb+\pacs{#1}+ command.
\end{description}
\end{abstract}

\pacs{Valid PACS appear here}% PACS, the Physics and Astronomy
                             % Classification Scheme.
%\keywords{Suggested keywords}%Use showkeys class option if keyword
                              %display desired
\maketitle

%\tableofcontents
\onecolumngrid
\subsection{Introduction}
\subsection{Probabilistic Description of Processes}
A probabilistic description of an overdamped Brownian particle in a harmonic potential (also called Ornstein-Uhlenbeck process) was first reported by Ornstein and Uhlenbeck in 1930 \cite{RN28}
\begin{equation}\label{OUp}
	x_{t+\Delta t} \sim \mathcal{N}(\mu=Bx_{t},\sigma^{2}=A(1-B^{2}))
\end{equation}
where $B(\Delta t) = \exp \left( { - \frac{\Delta t}{\tau}} \right)$ and $\mathcal{N}$ representing a normal distribution.
This equation describes the conditional probability of finding a particle at time $t+\Delta t$ at $x_{t+\Delta t}$ given that it was at $x_{t}$ at time $t$.  The likelihood function for a specific time trace, that is taken at intervals $\Delta t$, $\left\{x_i(i\Delta t)\right\}$ is:
\begin{equation}
	p\left( \left\{x_i(t_i)\right\} \left| B, A \right.\right) =
	\frac{1}{\sqrt {2 \pi A} }
	\exp \left( { - \frac{{x_1}^2}{2A}}\right)
	\frac{1}{{\sqrt {2\pi A(1-B^{2}(\Delta t))}^{(N-1)} }}
	\exp \left( { - \sum\limits_{i=1}^{N-1}\frac{{{{\left( {x_{i+1} - {x_i}B(\Delta t)} \right)}^2}}}{{2A(1-B^{2}(\Delta t))}}} \right)
\end{equation}
We recently published an anlystical solution to the Ornstein-Uhlenbeck maximum likelihood problem and here we want to consider the same problem but with added noise.  For the moment, we will assume Gaussian noise that is added to each data point.  Such noise can be described as follows:
\begin{equation}
	y_{i} \sim \mathcal{N}(\mu=x_{i},\sigma=\sigma_{N})
\end{equation}
with likelihood function for the measured data $\{y_{i}\}$ given $\{x_{i}\}$ and $\sigma_{N}$:
\begin{equation}
	p\left( \left\{y_i(t_i)\right\} \left| \left\{x_i(t_i)\right\},\sigma_{N} \right.\right) =
	\frac{1}{{\sqrt {2\pi \sigma_{N}^{2}}^{N} }}
	\exp \left( { - \sum\limits_{i=1}^{N}\frac{{{{\left( {y_{i} - x_{i}B(\Delta t)} \right)}^2}}}{{2\sigma^{2}}}} \right)
\end{equation}

Since we are measuring $\{y_{i}\}$, $\{x_{i}\}$ are latent variables.  If we knew $\{x_{i}\}$ and $\{y_{i}\}$ the likelihood function would simply be.
\begin{equation}
	p\left( \{x_{i}\},\{y_{i}\}|A,B,\sigma_{N}\right) = 
	p\left( \left\{y_i(t_i)\right\} \left| \left\{x_i(t_i)\right\},\sigma_{N} \right.\right)
	p\left( \left\{x_i(t_i)\right\} \left| B, A \right.\right)
\end{equation}
But since we don't observe $\{x_{i}\}$, we need to marginalize over $\{x_{i}\}$:
\begin{equation}
p\left( \{y_{i}\}|A,B,\sigma_{N}\right) = \idotsint p\left( \{x_{i}\},\{y_{i}\}|A,B,\sigma_{N}\right) \,dx_1 \dots dx_N
\end{equation}
Using Bayes Theorem, we can express the conditional probability of the parameters $\{A,B,\sigma_{N}\}$ given the measured data $\{y_{i}\}$.
\begin{equation}
	p\left( \{A,B,\sigma_{N}\}|\{y_{i}\},\right) =
	p\left( \{y_{i}\}|A,B,\sigma_{N}\right)p\left(A,B,\sigma_{N}\right)
\end{equation}
From this equation, we can estimate the parameters from a measurement of $\{y_{i}\}$ using a maximum likelihood approach.
\subsection{Implementation of the Expectation Maximization algorithm}
As we can see from the previous equation, a simple maximum likelihood approach is computationally challenging.  Because we are dealing with Gaussian distributions, a typical approach is to take the logarithm of the likelihood and hope that the likelihood factors nicely.  But in our case, because of the marginalization of the hidden variables $\{x_{i}\}$, we are taking the logarithm of integrals over $\{x_{i}\}$ which does not simplify the equation.  On the other hand, we recognize that $p\left( \{x_{i}\},\{y_{i}\}|A,B,\sigma_{N}\right)$ factors nicely.  The Expectation Maximization (EM) algorithm takes advantage of this fact.  The EM algorithm works as follows: (1) pick a starting point for the parameters $\Theta^{old} = \{A^{old},B^{old},\sigma_{N}^{old}\}$; (2) calculate the probability distribution for each hidden variable $\{x_{i}\}$ given these parameters; (3) maximize the following function with respect to $\Theta$ to find the new parameters.
\begin{equation}
	\mathcal{Q}(\Theta,\Theta^{old}) = \idotsint p\left( \{x_{i}\}|\{y_{i}\},\Theta^{old}\right)\ln p\left( \{x_{i}\},\{y_{i}\}|\Theta\right)\,dx_1 \dots dx_N
\end{equation}
(4) repeat by using $\Theta$ as $\Theta_{old}$ until convergence.  The EM alorithm converges, but it may find local maxima in the posterior distribution.  Therefore it is important to vary the starting point for $\Theta$.  To evaluate the integral in eq., we need to calculate $p\left( x_{n}|\{y_{i}\},\Theta^{old}\right)$.  We can again use Bayes rule and the conditional indpendence rules of a linear hidden Markov chain:
\begin{equation}
	\begin{aligned}
	p\left( x_{n}|\{y_{i}\},\Theta^{old}\right)&=\frac{p\left( \{y_{i}\}|x_{n},\Theta^{old}\right)p(x_{n})}{p(\{y_{i}\}|\Theta^{old})}\\
	&=\frac{p(y_{1},\dots,y_{n}|x_{n},\Theta^{old})p(y_{n+1},\dots,y_{N}|x_{n},\Theta^{old})p(x_{n})}{p(\{y_{i}\}|\Theta^{old})}\\
	&=\frac{p(y_{1},\dots,y_{n},x_{n}|\Theta^{old})p(y_{n+1},\dots,y_{N}|x_{n},\Theta^{old})}{p(\{y_{i}\}|\Theta^{old})}
	&=\frac{\alpha(x_{n})\beta(x_{n})}{p(\{y_{i}\}|\Theta^{old})}
	\end{aligned}
\end{equation}
using the following definition
\begin{equation}
	\begin{aligned}
	\alpha(x_{n})&=p(y_{1},\dots,y_{n},x_{n}|\Theta^{old})\\
	\beta(x_{n})&=p(y_{n+1},\dots,y_{N}|x_{n},\Theta^{old})
	\end{aligned}
\end{equation}
here we took advantage of the properties of the Markov chain $\{x_{i}\}$.  If we know a specific $x_{n}$ then the $y_{i}$ with $i\leq n$ are independent of $y_{i}$ with $i>n$.
\begin{acknowledgments}
We wish to acknowledge funding by the 
\end{acknowledgments}

\end{document}
