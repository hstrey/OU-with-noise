% ****** Start of file OUnoise.tex ******
%%
\documentclass[%
 reprint,
%superscriptaddress,
%groupedaddress,
%unsortedaddress,
%runinaddress,
%frontmatterverbose, 
%preprint,
%showpacs,preprintnumbers,
%nofootinbib,
%nobibnotes,
%bibnotes,
 amsmath,amssymb,
 aps,
%pra,
%prb,
%rmp,
%prstab,
%prstper,
%floatfix,
]{revtex4-1}

\usepackage{graphicx}% Include figure files
\usepackage{dcolumn}% Align table columns on decimal point
\usepackage{bm}% bold math
\usepackage{subcaption}
\usepackage{float}
\usepackage{tikz}
\usepackage{booktabs}
%\usepackage{breqn}
\usetikzlibrary{calc,arrows.meta,positioning}
%\usepackage{hyperref}% add hypertext capabilities
%\usepackage[mathlines]{lineno}% Enable numbering of text and display math
%\linenumbers\relax % Commence numbering lines

%\usepackage[showframe,%Uncomment any one of the following lines to test 
%%scale=0.7, marginratio={1:1, 2:3}, ignoreall,% default settings
%%text={7in,10in},centering,
%%margin=1.5in,
%%total={6.5in,8.75in}, top=1.2in, left=0.9in, includefoot,
%%height=10in,a5paper,hmargin={3cm,0.8in},
%]{geometry}
\DeclareMathOperator\erf{erf}
\DeclareMathOperator\erfc{erfc}
\begin{document}

\preprint{APS/123-QED}

\title{Parameter estimation for correlated Ornstein-Uhlenbeck time-series}

\author{Helmut H. Strey}
 \affiliation{Biomedical Engineering Department and Laufer Center for Physical and Quantitative Biology, Stony Brook University, Stony Brook NY 11794-5281.}%Lines break automatically or can be forced with \\
\author{Rajat Kumar}
 \affiliation{Biomedical Engineering Department, Stony Brook University, Stony Brook NY 11794-5281.}%Lines break automatically or can be forced with \\
\author{Lilianne Mujica-Parodi}
 \affiliation{Biomedical Engineering Department and Laufer Center for Physical and Quantitative Biology, Stony Brook University, Stony Brook NY 11794-5281.}
  \affiliation{Martinos Center, Massachusetts General Hospital and Harvard University, Boston, MA}
 %Lines break automatically or can be forced with \\

\date{\today}% It is always \today, today,
             %  but any date may be explicitly specified

\begin{abstract}
This article develops a Maximum likelihood (ML) approach to estimate parameters from correlated time traces that originate from coupled Ornstein-Uhlenbeck processes.  The most common technique to characterize the correlation between time-series is to calculate the Pearson correlation coefficient.  Here we show that our method gives more reliable results for time series with memory (or a characteristic relaxation time) and results in coupling coefficients and their uncertainties given the data.  We investigate how these uncertainties depend on the number of samples, the relaxation times, and sampling time.  We performed simulations over a wide range of correlation coefficients using our maximum likelihood solutions and Markov-Chain Monte-Carlo (MCMC) simulations to validate our analytic results.  We found that both ML and MCMC result in the same parameter estimations.  We also found that when analyzing the same data, the ML and MCMC uncertainties are strongly correlated. At the same time, ML underestimates the uncertainties by a factor of 1.5 to 3 over an extensive range of parameters.  For large datasets, we can therefore use the less computationally expensive maximum likelihood method to run over the whole dataset. Then we can use MCMC on a few samples to determine the factor by which the ML method underestimates the uncertainties.  To illustrate our method's application, we apply it to time series of brain activation using fMRI measurements of the human default mode network.  We show that our method significantly improves the interpretation of multi-subject measurements of correlations between brain regions by providing parameter confidence intervals for individual measurements, distinguishing between the variance from differences between subjects from variance due to measurement error.
\end{abstract}

\maketitle

%\tableofcontents
\onecolumngrid
\subsection{Introduction}
In many fields of science, a common question arises as to whether two sets of data are linearly correlated.  One of the most straightforward statistical tools for such a case is calculating the Pearson correlation coefficient $\rho$.  The Pearson correlation method assumes that the samples are identically and independently drawn from a bivariate Gaussian distribution.  Unfortunately, the Pearson coefficient is frequently applied to data that falls outside this condition. A pervasive example is the application of Pearson correlations to time-series that are distributed normally but where adjacent time points are not independent.  In this article, we are developing a method to characterize correlations between two time-series that exhibit short-term memory or relaxation.  In particular, we will restrict ourselves to the simplest random process that results in a fluctuating time series with a characteristic relaxation time: the Ornstein-Uhlenbeck (OU) process \cite{RN28}.  A OU process with mean zero can be expressed by the following Langevin equation:
\begin{equation}
\dot x =  - \frac{k}{\gamma }x + \frac{1}{\gamma }\xi(t)
\label{model}
\end{equation}
where k is the spring constant, $\gamma$ is the friction coefficient and $\xi(t)$ is a randomly fluctuating force.  
We and others have recently published maximum likelihood methods to estimate parameters from an OU process time series \cite{RN91,RN51,RN62}, and here we attempt the same for coupled OU processes.
\subsection{Coupled Oscillators}\label{sec_CO}
We consider a system of two overdamped oscillators, characterized by a friction coefficient $\gamma$ and spring constant $k$, that are coupled by a spring with a spring constant $c$.  The Langevin equations can be written in the following way:
\begin{equation}
	\begin{aligned}
		\dot{x_1} &= -\frac{k}{\gamma}x_{1}-\frac{c}{\gamma}(x_{1}-x_{2}) +\frac{1}{\gamma}\xi_{1}(t)\\
		\dot{x_2} &= -\frac{k}{\gamma}x_{2}-\frac{c}{\gamma}(x_{2}-x_{1}) +\frac{1}{\gamma}\xi_{2}(t)
	\end{aligned}
\end{equation}
This system can be decoupled by the two eigenfunctions $2y_{1}=x_{1}+x_{2}$ and $2y_{2}=x_{1}-x_{2}$, which results in
\begin{equation}\label{eq_decoupled}
	\begin{aligned}
		\dot{y_1} &= -\frac{k}{\gamma}y_{1} +\frac{1}{\gamma}\xi_{3}(t)\\
		\dot{y_2} &= -\frac{k+c}{\gamma}y_{2} +\frac{1}{\gamma}\xi_{4}(t)
	\end{aligned}
\end{equation}
where $y_{1}(t)$ and $y_{2}(t)$ are two Ornstein-Uhlenbeck processes with different relaxation rates.  The equipartition theorem yields $<y_{1}^{2}> = A_{1} = \frac{k_{B}T}{k}$ and $<y_{2}^{2}> = A_{2} = \frac{k_{B}T}{k+c}$.  The other important relationships are: diffusion coefficient $D=\frac{k_{B}T}{\gamma}$, relaxation times $\tau_{1}=\frac{A_{1}}{D}$ and $\tau_{2}=\frac{A_{2}}{D}$. If the friction coefficients $\gamma$ are different for $x_{1}$ and $x_{2}$ one can still decouple the system of differential equations by finding the eigenvectors and eigenvalues \cite{RN95}.  For simplicity we assume that both processes have the same friction coefficient or diffusion coefficient $D$.

In many fields, the Pearson Correlation Coefficient $\rho$ is used to characterize the correlation between two time-series.  The connection between our coupling coefficient $c$ and $\rho$ is:
\begin{equation}
	\rho = \frac{\left<x_{1}x_{2}\right>}{\sigma_{1}\sigma_{2}} = \frac{\left<(y_{1}+y_{2})(y_{1}-y_{2})\right>}{\sqrt{\left<(y_{1}+y_{2})^{2}\right>\left<(y_{1}-y_{2})^{2}\right>}} = \frac{\left<y_{1}^{2}\right>-\left<y_{2}^{2}\right>}{\left<y_{1}^{2}\right>+\left<y_{2}^{2}\right>} = \frac{c}{2k+c}
\end{equation}
As expected, for $c=0$ we obtain $\rho=0$, and $c=\infty$ results in $\rho=1$. Often correlation analysis are performed on normalized samples.  It therefore makes sense to define a unitless coupling coefficient $C=c/k$  which can be calculated by estimating $A_1$ and $A_2$ from the data:
\begin{equation}\label{Crho}
	C = \frac{A_{1}-A_{2}}{A_{2}}=\frac{c}{k}
\end{equation}
We can extend this procedure to negative coupling coefficients.  For example, $c=-k$ would results in $\rho=-1$, but at this $c$ (see eq. \ref{eq_decoupled}) $y_2$ becomes unbound.  In order to map the normalized coupling coefficient C in the range from $-\infty$ to $+\infty$ to the Pearson correlation coefficient $\rho=-1$ to $\rho=1$ we suggest the following procedure:  From two normalized samples $x_1$ and $x_2$, we calculate $2y_{1}=x_{1}-x_{2}$ and $2y_{2}=x_{1}-x_{2}$.  From $y_1$ and $y_2$, we estimate the amplitudes $A_{1}$ and $A_{2}$.  If $A_{1}>A_{2}$ then the coupling coefficient $C>0$ and can be calculated using $C=(A_{1}-A_{2})/A_{2}$.  If on the other hand $A_{1}<A_{2}$ then the coupling coefficent is negative and $C=(A_{1}-A_{2})/A_{1}$.  Reversing the process, we can use this procedure to create artificial time-series, by simulating two independent OU processes (eq. \ref{eq_decoupled}) to produce two coupled OU processes $x_{1}=y_{1}+y_{2}$ and $x_{2}=y_{1}-y_{2}$ with a well-defined positive coupling $C$ or $\rho$.  One obtains negative coupling coefficients by using $x_{2}=y_{2}-y{1}$.

\begin{figure}[H]
\begin{center}
\resizebox{.3\textwidth}{!}{%
\includegraphics[height=3cm]{corr.png}%
}
\caption{Relationship between coupling coefficient $C$ and Pearson correlation coefficient $\rho$.  The solid line represents eq. \ref{Crho}, whereas the data points represent the parameter estimation by MCMC simulation on artificial data of length $N=1000$.  The errorbars indicate the distribution of estimated $C$s.}\label{fig:corr}
\end{center}
\end{figure}

\subsection{Maximum likelihood solution}
In this section we will express the likelihood (see for example \cite{RN42}) of two correlated time-series $x_1$ and $x_2$ in terms of $A_{1}$, $A_{2}$ and $B_{k}\equiv B(\Delta t,D,A_{k})=\exp\left(-\frac{D}{A_{k}}\Delta t\right)$ assuming uniform priors for all parameters.
\begin{eqnarray}
	&&p\left( \left\{x_{1,i}(t_i)\right\},\left\{x_{2,i}(t_i)\right\} \left| D, A_{1},A_{2} \right.\right) \propto
	\frac{1}{\sqrt {2 \pi A_{1}}^{N} }
	\frac{1}{{\sqrt {(1-B_{1}^{2})}^{(N-1)} }}\\
	&&\times\exp \left( -\frac{1}{2A_{1}}\left( {x_{1,1}}^{2} + 
	\sum\limits_{i=1}^{N-1}\frac{ x_{1,i+1}^{2} - 2x_{1,i+1}x_{1,i}B_{1} +x_{1,i}^{2}B_{1}^{2} }{(1-B_{1}^{2})} \right)\right)\nonumber\\
	&&\times\frac{1}{\sqrt {2 \pi A_{2}}^{N} }
	\frac{1}{{\sqrt {(1-B_{2}^{2})}^{(N-1)} }}\\
	&&\times\exp \left( -\frac{1}{2A_{2}}\left( {x_{2,1}}^{2} + 
	\sum\limits_{i=1}^{N-1}\frac{ x_{2,i+1}^{2} - 2x_{2,i+1}x_{1,i}B_{2} +x_{2,i}^{2}B_{2}^{2} }{(1-B_{2}^{2})} \right)\right)\nonumber
\end{eqnarray}
In order to find the maximum likelihood it is convenient to take the logarithm of $p$ and then take the derivatives with respect to $A_{1}$, $A_{2}$ and $D$.
\begin{equation}
\begin{aligned}
	\Phi &= ln \left( p\left( \left\{x_{1,i}(t_i)\right\},\left\{x_{2,i}(t_i)\right\} \left|  D, A_{1},A_{2} \right.\right) \right)\\\\
	&= C - \frac{N}{2} ln(A_{1}) - \frac{N}{2} ln(A_{2})- \frac{N-1}{2}ln \left( 1-B_{1}^{2}\right) - \frac{N-1}{2}ln \left( 1-B_{2}^{2}\right) -\frac{1}{2A_{1}}Q_{1}(B_{1})-\frac{1}{2A_{2}}Q_{2}(B_{2})
\end{aligned}
\end{equation}
with\begin{equation}
	\begin{aligned}
	&Q_{k}(B_{k}) = {x_{k,1}}^{2} + \sum\limits_{i=1}^{N-1}\frac{ {x_{k,i+1}^{2} - 2x_{k,i+1}{x_i}B_{k}} +{x_i}^{2}B_{k}^{2} }{(1-B_{k}^{2})}\\
	&= \frac{x_{k,1}^{2}+x_{1,N}^{2}}{1-B_{k}^2}+\frac{1+B_{k}^2}{1-B_{k}^2}\sum\limits_{i=2}^{N-1}x_{k,i}^{2}-\frac{2B_{k}}{1-B_{k}^2}\sum\limits_{i=1}^{N-1}x_{k,i}x_{k,i+1}
	\end{aligned}
\end{equation}
with $C$ representing an unimportant constant. $Q_{k}(B_{k})$ reveals the fundamental statistic - the only terms that exclusively contain the data $\{x_{k,i}\}$ with $k=1,2$:
\begin{equation}
	\begin{aligned}
		a_{k,EndPoints}&=a_{k,EP}=x_{k,1}^{2}+x_{k,N}^{2}\\
		a_{k,SumSquared}&=a_{k,SS}=\sum\limits_{i=2}^{N-1}x_{k,i}^{2}\\
		a_{k,Correlation}&=a_{k,C}=\sum\limits_{i=1}^{N-1}x_{k,i}x_{k,i+1}\\
	\end{aligned}
\end{equation}
We find the maximum likelihood parameters $A_{k,max}$ and $D_{max}$ by finding the roots of the first derivatives with respect to the three parameters.  The derivative with respect to $A_{k}$ results in the following two conditions:
\begin{eqnarray}\label{partialsigma}
	&&\left.\frac{\partial}{\partial A_{k}}\Phi\right|_{A_{k,max},D_{max}} = -\frac{N}{2A_{k,max}}
	+\frac{1}{2A_{k,max}^{2}}Q(B_{k,max})\\
	&&+\frac{\partial B_{k}}{\partial A_{k}}\left.\left(\frac{B_{k,max}(N-1)}{1-B_{k,max}^2}
	 -\frac{1}{2A_{k,max}}
	 \frac{\partial Q(B_{k})}{\partial B_{k}}
	 \right)\right|_{A_{k,max},D_{max}}=0
\end{eqnarray}
with
\begin{equation}
	\frac{\partial}{\partial B_{k}}Q_{k}(B_{k}) =  \frac{2}{(1-B_{k}^{2})^{2}}\left(B_{k}a_{k,EP}+2B_{k}a_{k,SS}-(1+B_{k}^{2})a_{k,C}\right)
\end{equation}
and
\begin{equation}
	\frac{\partial B_{k}}{\partial A_{k}} = \frac{B_{k}D\Delta t}{A_{k}^{2}}
\end{equation}
The derivative with respect to $D$ can be written:
\begin{eqnarray}\label{partiald}
	&&\left.\frac{\partial}{\partial D}\Phi\right|_{A_{k,max},D_{max}} =\\
	&&\left(\frac{B_{1}(N-1)}{1-B_{1}^{2}}
	-\frac{1}{2A_{1}}\frac{\partial}{\partial B_{1}}Q_{1}(B_{1})\right)\left.\frac{\partial B_{1}}{\partial D}\right|_{A_{1,max},D_{max}}\\
	&&+\left(\frac{B_{2}(N-1)}{1-B_{2}^{2}}
	-\frac{1}{2A_{2}}\frac{\partial}{\partial B_{2}}Q_{2}(B_{2})\right)\left.\frac{\partial B_{2}}{\partial D}\right|_{A_{2,max},D_{max}}=0
\end{eqnarray}
with
\begin{equation}
	\frac{\partial B_{k}}{\partial D} = -B_{k}\frac{\Delta t}{A_{k}}
\end{equation}
From these three equations we need to find $A_{1}$,$A_{2}$ and $D$, which can be done using numerical root finding algorithms that are implemented in standard numerical libraries (e.g. Powell hybrid method implemented in MINPACK \cite{osti_6997568}).  In order to estimate the parameter uncertainties, we need to calculate the Jacobian.  Here the only non-zero terms are: 

\begin{equation}
	\begin{aligned}
		\frac{\partial^{2}}{\partial A_{k}^2}\Phi &= \frac{N}{2A_{k}^{2}} - \frac{Q_{k}(B_{k})}{A_{k}^{3}}
		+ \frac{N-1}{1-B_{k}^{2}}\left(B_{k}\frac{\partial^{2} B_{k}}{\partial A_{k}^{2}}
		+ \left(\frac{\partial B_{k}}{\partial A_{k}}\right)^{2}
		\frac{1+B_{k}^{2}}{1-B_{k}^{2}}\right)
		-\frac{1}{2A_{k}}\frac{\partial^{2}Q_{k}(B_{k})}{\partial A_{k}^{2}}
		+\frac{1}{A_{k}^{2}}\frac{\partial Q_{k}(B_{k})}{\partial A_{k}}\\
		&=-\frac{N}{2A_{k}^{2}}
		+ \frac{N-1}{1-B_{k}^{2}}\left(B_{k}\frac{\partial^{2} B_{k}}{\partial A_{k}^{2}}
		+ \left(\frac{\partial B_{k}}{\partial A_{k}}\right)^{2}
		\frac{1+B_{k}^{2}}{1-B_{k}^{2}}+\frac{2B_{k}}{A_{k}}\frac{\partial B_{k}}{\partial A_{k}}\right)
		-\frac{1}{2A_{k}}\frac{\partial^{2}Q_{k}(B_{k})}{\partial A_{k}^{2}}\\
		\frac{\partial^{2}}{\partial A_{k}\partial D}\Phi &= \frac{1}{2A_{k}^{2}}\frac{\partial Q_{k}(B_{k})}{\partial D}
		-\frac{1}{2A_{k}}\frac{\partial^{2} Q_{k}(B_{k})}{\partial A_{k}\partial D}
		+ \frac{N-1}{1-B_{k}^{2}}\left(B_{k}\frac{\partial^{2} B_{k}}{\partial A_{k}\partial D}
		+ \frac{\partial B_{k}}{\partial A_{k}}\frac{\partial B_{k}}{\partial D}
		\frac{1+B_{k}^{2}}{1-B_{k}^{2}}
		\right)\\
		\frac{\partial^{2}}{\partial D^2}\Phi &=
		\frac{N-1}{1-B_{1}^{2}}\left(B_{1}\frac{\partial^{2} B_{1}}{\partial D^{2}}
		+ \left(\frac{\partial B_{1}}{\partial D_{k}}\right)^{2}
		\frac{1+B_{k}^{2}}{1-B_{k}^{2}}
		\right)
		+\frac{N-1}{1-B_{2}^{2}}\left(B_{2}\frac{\partial^{2} B_{2}}{\partial D^{2}}
		+ \left(\frac{\partial B_{2}}{\partial D}\right)^{2}
		\frac{1+B_{k}^{2}}{1-B_{k}^{2}}
		\right)\\
		&- \frac{1}{2A_{1}}\frac{\partial^{2}Q_{1}(B_{1})}{\partial D^{2}}
		- \frac{1}{2A_{2}}\frac{\partial^{2}Q_{2}(B_{2})}{\partial D^{2}}
	\end{aligned}
\end{equation}
with
\begin{equation}
	\begin{aligned}
		\frac{\partial^{2} B_{k}}{\partial A_{k}^{2}} &=\frac{B_{k}D\Delta t}{A_{k}^{3}}\left(\frac{D\Delta t}{A_{k}}-2\right) \\
		\frac{\partial^{2} B_{k}}{\partial A_{k}\partial D} & = \frac{B_{k}\Delta t}{A_{k}^{2}}\left(1-\frac{D\Delta t}{A_{k}}\right)\\
		\frac{\partial^{2} B_{k}}{\partial D^{2}} &= B_{k}\frac{\Delta t^{2}}{A_{k}^{2}}\\
		\frac{\partial^{2} Q_{k}(B_{k})}{\partial A_{k}\partial D} &= 
		\frac{\partial^{2} Q_{k}(B_{k})}{\partial B_{k}^{2}}\frac{\partial B_{k}}{\partial D}\frac{\partial B_{k}}{\partial A_{k}}
		+\frac{\partial Q_{k}(B_{k})}{\partial B_{k}}\frac{\partial^{2} B_{k}}{\partial A_{k}\partial D}\\
		\frac{\partial^{2}Q_{k}(B_{k})}{\partial D^{2}} &= \frac{\partial^{2}Q_{k}(B_{k})}{\partial B_{k}^{2}}\left(\frac{\partial B_{k}}{\partial D}\right)^{2}
		+\frac{\partial Q_{k}(B_{k})}{\partial B_{k}}\frac{\partial^{2}B_{k}}{\partial D^{2}}\\
		\frac{\partial^{2}Q_{k}(B_{k})}{\partial A_{k}^{2}} &= 
		\frac{\partial^{2}Q_{k}(B_{k})}{\partial B_{k}^{2}}\left(\frac{\partial B_{k}}{\partial A_{k}}\right)^{2}
		+\frac{\partial Q_{k}(B_{k})}{\partial B_{k}}\frac{\partial^{2}B_{k}}{\partial A_{k}^{2}}\\
		\frac{\partial^{2}Q_{k}(B_{k})}{\partial B_{k}^{2}} &= \frac{6B_{k}+2}{(1-B_{k}^{2})^{3}}\left(a_{k,EP}+2a_{k,SS}\right)
		-\frac{4B_{k}^{3}+12B_{k}}{(1-B_{k}^{2})^{3}}a_{k,C}
	\end{aligned}
\end{equation}
Using these terms, we can now express the Hessian $H$ as:
\begin{equation}H=
\begin{bmatrix} 
\frac{\partial^{2}}{\partial A_{1}^2}\Phi & 0 & \frac{\partial^{2}}{\partial A_{1}\partial D}\Phi \\
0 & \frac{\partial^{2}}{\partial A_{2}^2}\Phi & \frac{\partial^{2}}{\partial A_{2}\partial D}\Phi \\
\frac{\partial^{2}}{\partial A_{1}\partial D}\Phi & \frac{\partial^{2}}{\partial A_{2}\partial D}\Phi & \frac{\partial^{2}}{\partial D^2}\Phi\\
\end{bmatrix}
\end{equation}
whose negative inverse determines the parameter's covariance matrix $C = -H^{-1}(A_{1,max},A_{2,max},D_{max})$ (see for example \cite{RN42}). In order to get a better sense of how the parameter estimation errors behave we can calculate the expectation values of the second derivaties considering $N\rightarrow \infty$: $\left\langle a_{k,EP} \right\rangle=2A_{k}$,$\left\langle a_{k,SS} \right\rangle=(N-2)A_{k}$, and $\left\langle a_{k,C} \right\rangle=(N-1)A_{k}B_{k}$.  This results in the following expectation values of the derivatives of $Q_{k}(B_{k})$:
\begin{equation}
	\begin{aligned}
	\left\langle Q_{k}(B_{k})\right\rangle &= NA_{k}\\
	\left\langle\frac{\partial}{\partial B_{k}}Q_{k}(B_{k})\right\rangle &=  \frac{2NA_{k}B_{k}}{1-B_{k}^{2}}\\
	\left\langle\frac{\partial^{2}Q_{k}(B_{k})}{\partial B_{k}^{2}}\right\rangle &= 
	-\frac{4NA_{k}}{(1-B_{k}^{2})^{3}}\left(B_{k}^{4}+3B_{k}^{2}-3B_{k}-1\right)
	\end{aligned}
\end{equation}
From this, we can express the Hessian using the following second derivatives:
\begin{equation}\label{eqNinfty}
	\begin{aligned}
		\frac{\partial^{2}}{\partial A_{k}^2}\Phi &= -\frac{N}{2A_{k}^{2}} 
		- \frac{N}{(1-B_{k}^{2})^{2}}\left(\frac{\partial B_{k}}{\partial A_{k}}\right)^{2}
		\left(1+B_{k}^{2}+\frac{6B_{k}}{1+B_{k}}\right)+\frac{2NB_{k}}{A_{k}(1-B_{k}^{2})}\frac{\partial B_{k}}{\partial A_{k}}\\
		\frac{\partial^{2}}{\partial A_{k}\partial D}\Phi &=
		\frac{NB_{k}}{A_{k}(1-B_{k}^{2})}\frac{\partial B_{k}}{\partial D}
		-\frac{N}{(1-B_{k}^{2})^{2}}\frac{\partial B_{k}}{\partial A_{k}}\frac{\partial B_{k}}{\partial D}
		\left(1+B_{k}^{2}+\frac{6B_{k}}{1+B_{k}}\right)\\
		\frac{\partial^{2}}{\partial D^2}\Phi &=
		- \frac{N}{(1-B_{1}^{2})^{2}}\left(\frac{\partial B_{1}}{\partial D}\right)^{2}
		\left(1+B_{1}^{2}+\frac{6B_{1}}{1+B_{1}}\right)
		- \frac{N}{(1-B_{2}^{2})^{2}}\left(\frac{\partial B_{2}}{\partial D}\right)^{2}
		\left(1+B_{2}^{2}+\frac{6B_{2}}{1+B_{2}}\right)\\
	\end{aligned}
\end{equation}
These equations allow us to express how the uncertainties/errors of the parameters depend on the sampling interval $\Delta t/\tau$ given that we measure $N$ samples.  In Fig. \ref{fig:error_fig} we plot the relative errors of $A_{1},A_{2}$ and $C$ as a function of $\Delta t/\tau_{1}$ and $\rho$ for large fixed $N$ ($N=10^{6}$) using eqs. \ref{eqNinfty}.  For example, in Fig.\ref{fig:error_A} we can see that the optimal sampling interval as given by $\Delta t/\tau_{1}$ is reducing as $\rho$ is increasing.  This is most likely due to the fact that the relaxation $\tau_{2}$ is decreasing as $\rho$ is increasing which will affect the uncertainty of both $A_{1}$ and $A_{2}$.  We also observe a shift in behavior for larger $\rho>0.25$ where no minimum $\Delta t/\tau_{1}$ for $A_{1}$ and $C$ exists.  This is in contrast to our finding for a single OU process \cite{RN91} where the relaxation time $\tau$ had a minimum of approximatly $\Delta t = 0.7\tau$ and $dA$ continuously decreased as $\Delta t/\tau$ decreased.
\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{0.4\textwidth}
        \includegraphics[width=\textwidth]{dA.png}
        \caption{Amplitude errors $dA1$ and $dA2$ as function of $\Delta t/\tau$}
        \label{fig:error_A}
    \end{subfigure}
    ~ %add desired spacing between images, e. g. ~, \quad, \qquad, \hfill etc. 
      %(or a blank line to force the subfigure onto a new line)
    \begin{subfigure}[b]{0.4\textwidth}
        \includegraphics[width=\textwidth]{dC.png}
        \caption{Relative coupling coefficient error $dC$ as function of $\Delta t/\tau$}
        \label{fig:error_C}
    \end{subfigure}
    \caption{Dependence of relative errors on $\Delta t/\tau$ for large $N$ ($N=10^{6}$) using eqs. \ref{eqNinfty}}\label{fig:error_fig}
\end{figure}
\subsection{Validation by Simulation}
In this section, we create artifical correlated time-series employing the method mentioned in \ref{sec_CO} to validate our Maximum-Likelihood method against parameter estimation by Markov-Chain Monte-Carlo (MCMC) methods \cite{RN46}.  Probabilistic calculations using MCMC were carried out in python and Julia, using the anaconda distribution of python \cite{anaconda} with PYMC3 \cite{RN75} and Julia 1.53 \cite{bezanson2017julia} with Turing.jl \cite{ge2018t}.  For the following table and figures, we created 400 correlated time-series pairs for Pearson Correlation coefficients of $0.9$, $0.5$, $0.25$, and $0.1$ for $N=1000$ at $D=1,A_{1}=1,\Delta t = 0.3$.  Since the problem is symmetric with respect to the sign of the correlation coefficient we don't need to simulate negative correlation coefficients. Fig. \ref{fig:Arho05} shows the proability distributions of $A_1$ and $A_2$ for $N=1000$ and $\rho=0.5$ estimated by maximum likelihood (ML) and Markov-Chain Monte-Carlo (MCMC). The results of the simulations are summarized in Table \ref{simtable}.  In terms of uncertainties we find that MCMC correctly estimates parameter uncertainties consistent with the distribution over a large sample, but that the maximum likelihood method underestimates the uncertainty by a constant factor that depends on the degree of correlation $\rho$ or $C$. In Fig. \ref{fig:Acorr} we show using a scatter plot that the individual uncertainties of MCMC and ML are correlated.  This fact is important since MCMC is much more computationally expensive.  For large datasets, we can use the maximum likelihood method to run over the whole dataset, and then we can use MCMC on a few samples to determine the factor by which the ML method underestimates the uncertainties.  In our example, maximum likelihood underestimates the uncertainties by a factor of 1.5-2 over a wide range of $\rho$.  We believe this underestimation results from the fact that the distributions in Fig. \ref{fig:Arho05} are not Gaussian.  The ML method assumes that all posterior distributions are Gaussian and that the width can be determined by the second derivative of the log likelihood.  This is clearly not the case.  The fact that we underestimate the uncertainties indicates that the curvature of our distribution at the maximum is greater than a corresponding Gaussian distribution.
\begin{table}
\begin{tabular}{l|c|c|c|c|c|c|c|c}
	\hline
	$\rho$     & $A_1$ (MCMC) & $A_1$ (ML) & $A_2$ (MCMC) & $A_2$ (ML) & $C$ (MCMC) &  $C$ (ML) & $dC$ (MCMC) &  $dC$ (ML)\\
	\hline
	0.1  & $1.02\pm0.09$ & $1.00\pm0.08$ & $0.84\pm0.07$ & $0.82\pm0.06$ & $0.24\pm0.15$ & $0.23\pm0.14$ & $0.14\pm0.018$  & $0.068\pm0.0076$ \\
	0.25 & $1.02\pm0.09$ & $1.00\pm0.08$ & $0.61\pm0.04$ & $0.60\pm0.04$ & $0.70\pm0.18$    & $0.67\pm0.17$   & $0.19\pm0.02$   & $0.09\pm0.009$\\
	0.5 & $1.02\pm0.08$  & $1.00\pm0.08$     & $0.33\pm0.02$    &  $0.33\pm0.02$   & $2.00\pm0.29$ & $2.05\pm0.30$ & $0.31\pm0.039$ & $0.144\pm0.013$ \\
	0.75  & $1.02\pm0.08$ & $1.00\pm0.08$ & $0.145\pm0.006$  & $0.144\pm0.005$ & $6.1\pm0.6$ & $6.0\pm0.6$ & $0.69\pm0.08$  & $0.37\pm0.03$\\
	0.9  & $1.02\pm0.08$ & $1.00\pm0.08$ & $0.053\pm0.002$ & $0.053\pm0.00003$ & $18.4\pm1.9$ & $17.97\pm1.5$ & $1.9\pm0.24$ & $1.3\pm0.11$\\
	\hline
\end{tabular}
\caption{Comparison of parameter estimates using maximum likelihood and Markov-Chain Monte-Carlo methods for different $\rho$ at $D=1,A_{1}=1,\Delta t = 0.3,N=1000$}
\label{simtable}
\end{table}

\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{0.4\textwidth}
        \includegraphics[width=\textwidth]{A1kde05.png}
        \caption{$p(A_{1})$ vs $A_1$}
        \label{fig:A1rho05}
    \end{subfigure}
    ~ %add desired spacing between images, e. g. ~, \quad, \qquad, \hfill etc. 
      %(or a blank line to force the subfigure onto a new line)
    \begin{subfigure}[b]{0.4\textwidth}
        \includegraphics[width=\textwidth]{A2kde05.png}
        \caption{$p(A_{2})$ vs $A_2$}
        \label{fig:A2rho05}
    \end{subfigure}
    \caption{Comparison of MCMC and maximum likelihood for $\rho=0.5$ and $D=1,A_{1}=1,\Delta t = 0.3,N=1000$.  The vertical bar indicates the true expected value}\label{fig:Arho05}
\end{figure}
\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{0.4\textwidth}
        \includegraphics[width=\textwidth]{dA1corrkde05.png}
        \caption{$dA_{1,ML}$ vs $dA_{1,MCMC}$}
        \label{fig:A1corr}
    \end{subfigure}
    ~ %add desired spacing between images, e. g. ~, \quad, \qquad, \hfill etc. 
      %(or a blank line to force the subfigure onto a new line)
    \begin{subfigure}[b]{0.4\textwidth}
        \includegraphics[width=\textwidth]{dA2corrkde05.png}
        \caption{$dA_{2,ML}$ vs $dA_{2,MCMC}$}
        \label{fig:A2corr}
    \end{subfigure}
    \caption{Correlation between the uncertainties of A estimated by maximum likelihood and MCMC for $\rho=0.5$ and $D=1,A_{1}=1,\Delta t = 0.3,N=1000$.  The dotted line represents a linear fit.  For this $\rho$ the maximum likelihood underestimates the errors by a factor of 2}\label{fig:Acorr}
\end{figure}

\subsection{Application to resting-state functional Magnetic Resonance Imaging}
To illustrate our correlation method's power, we analyzed functional Magnetic Resonance Imaging (fMRI) time-series data from control 7T-fMRI resting-state experiments that we took from \cite{RN98}.  We extracted neural activation time-series from four brain regions that make up the default mode network \cite{RN99} ( LLP = left lateral parietal cortex, RLP = right lateral parietal cortex, MPFC = medial prefrontal cortex, PCC = posterior cingulate cortex).  The default mode network activates when individuals are focused on their internal mental-state processes, such as self-referential processing, interoception etc. and not performing a specific task.  Fig. \ref{fig:subject4} shows the time series of a single subject.  Traditionally, correlations between brain regions are evaluated using Pearson correlation coefficients.  To ensure that our coupled OU model is applicable, we determined the skewness and kurtosis of all time-series to show that they are Gaussian.  None of the distributions had an absolute value of skewness/kurtosis of more than $0.5$, which is considered safe for applying Gaussian statistics (see for example \cite{RN100}).  Next, we have to show that the time series exhibit an exponentially decaying autocorrelation function.  For this, we applied our maximum likelihood method for OU processes to determine the relaxation times for all time-series.  We found that $\tau_{MPFC}=4.8 \pm 0.7 sec$, $\tau_{PCC}=4.8 \pm 0.7 sec$, $\tau_{RLP}=4.1 \pm 0.5 sec$, $\tau_{LLP}=3.8 \pm 0.5 sec$.  The decay-times of all brain regions are within one or two standard deviations of each other, justifying the assumption made in eq. \ref{eq_decoupled}.
Fig. \ref{fig:coupling_coef} shows the coupling coefficients of five subjects estimated by our method.  Here we show the estimates from MCMC, but we obtain the same mean values using the maximum likelihood method with uncertainties about a factor of 3 smaller than the MCMC estimates (see the Discussion in Simulation section).  We chose four of the six possible pairings between brain regions to illustrate the additional information that our method provides over Pearson correlations.  By obtaining the strength of the coupling and their confidence intervals given the data, we can distinguish between variance due to measurement error and variance due to differences between subjects.  Panel (a) illustrates a case where the measurements' error is smaller than the variance of the mean.  In this case, this is due to an outlier.  Here a traditional analysis would have easily detected the outlier.  In Panel (b) and (d), the variance of the mean is comparable to the variances of the individual data points.  Since we know each measurement's uncertainty, we can determine how much of the variance of the mean is due to the variance between subjects and measurement error.  Panel (c) illustrates a case where the measurement's uncertainty is larger than the variance of the mean.

\begin{figure}[H]
\begin{center}
\resizebox{.8\textwidth}{!}{%
\includegraphics[height=10cm]{subject4.png}%
}
\caption{Time series of fMRI-BOLD signals of four brain regions that constitute the default mode network ( LLP = left lateral parietal cortex, RLP = right lateral parietal cortex, MPFC = medial prefrontal cortex, PCC = posterior cingulate cortex) }\label{fig:subject4}
\end{center}
\end{figure}
\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{0.4\textwidth}
        \includegraphics[width=\textwidth]{mpfc_llpMCMC.png}
        \caption{Coupling coefficients between MPFC and LLP}
        \label{fig:mpfc_llp}
    \end{subfigure}
    ~ %add desired spacing between images, e. g. ~, \quad, \qquad, \hfill etc. 
      %(or a blank line to force the subfigure onto a new line)
    \begin{subfigure}[b]{0.4\textwidth}
        \includegraphics[width=\textwidth]{mpfc_pccMCMC.png}
        \caption{Coupling coefficients between MPFC and PCC}
        \label{fig:mpfc_pcc}
    \end{subfigure}
    \begin{subfigure}[b]{0.4\textwidth}
        \includegraphics[width=\textwidth]{pcc_llpMCMC.png}
        \caption{Coupling coefficients between PCC and LLP}
        \label{fig:llp_pcc}
    \end{subfigure}
    \begin{subfigure}[b]{0.4\textwidth}
        \includegraphics[width=\textwidth]{llp_rlpMCMC.png}
        \caption{Coupling coefficients between LLP and RLP}
        \label{fig:rlp_llp}
    \end{subfigure}
	\caption{Coupling coefficients between default mode network brain regions across several subjects and the mean and standard deviation between subjects.  For comparison to Pearson correlation coefficients we can use $\rho = C/(2+C)$ ($\rho=1/3$ for $C=1$, $\rho=0.6$ for $C=3$,  and $\rho=0.75$ for $C=6$).  The coupling coefficients and their uncertainties were determined by MCMC.  We obtained the same mean coupling coefficients using the maximum likelihood method but with uncertainties that are about a factor of 3 smaller than the MCMC estimate.}\label{fig:coupling_coef}
\end{figure}
\subsection{Summary}
In the previous sections we developed and validated a maximum likelihood method to estimate the correlation between time series with an intrinsic relaxation time (OU process).  Our approach improves Pearson correlations by providing robust estimates for the confidence interval of the coupling(correlation) coefficents.  Such confidence intervals enable us to distinguish between measurement error (confidence interval of a single measurement) and the variance that results from the natural variability of the coupling.  In the biological sciences, we often encounter large biological variance that manifests itself in cell-cell variability of genetically identical cells; or variability of the brain functional connectivity between (different subjects) but also within subjects (same subject at different days).  Only if the individual error of a measurement is known can we deconvolute the various sources of variance.  We hope that our method will help in achieving this goal.
\begin{acknowledgments}
We wish to acknowledge funding by the the National Institute of Drug Abuse (SBIR Phase 1 \& Phase 2 Grant 1R44 DA043277-01) and the National Heart, Lung and Blood Institute (Grant 5U01HL12752202)
\end{acknowledgments}
\bibliography{correlations}
\end{document}
